{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House price prediciton regression - TFX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-5b58e9f01123>, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-5b58e9f01123>\"\u001b[0;36m, line \u001b[0;32m18\u001b[0m\n\u001b[0;31m    from tfx.components import ResolverNodehttps://raw.githubusercontent.com/fabiansd/AI-workshop/master/data/melbourne_train.csv\u001b[0m\n\u001b[0m                                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import pprint\n",
    "import tempfile\n",
    "import urllib\n",
    "\n",
    "import absl\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_analysis as tfma\n",
    "tf.get_logger().propagate = False\n",
    "pp = pprint.PrettyPrinter()\n",
    "\n",
    "import tfx\n",
    "from tfx.components import CsvExampleGen\n",
    "from tfx.components import Evaluator\n",
    "from tfx.components import ExampleValidator\n",
    "from tfx.components import Pusher\n",
    "from tfx.components import ResolverNodehttps\n",
    "from tfx.components import SchemaGen\n",
    "from tfx.components import StatisticsGen\n",
    "from tfx.components import Trainer\n",
    "from tfx.components import Transform\n",
    "from tfx.components.base import executor_spec\n",
    "from tfx.components.trainer.executor import GenericExecutor\n",
    "from tfx.dsl.experimental import latest_blessed_model_resolver\n",
    "from tfx.orchestration import metadata\n",
    "from tfx.orchestration import pipeline\n",
    "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
    "from tfx.proto import pusher_pb2\n",
    "from tfx.proto import trainer_pb2\n",
    "from tfx.types import Channel\n",
    "from tfx.types.standard_artifacts import Model\n",
    "from tfx.types.standard_artifacts import ModelBlessing\n",
    "from tfx.utils.dsl_utils import external_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TensorFlow version: {}'.format(tf.__version__))\n",
    "print('TFX version: {}'.format(tfx.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the root directory for your TFX pip package installation.\n",
    "_tfx_root = pathlib.Path().parent.absolute().parent\n",
    "\n",
    "# This is the directory containing the TFX Chicago Taxi Pipeline example.\n",
    "_housepred_root = _tfx_root.joinpath('tfx_metadata_store')\n",
    "\n",
    "# This is the path where your model will be pushed for serving.\n",
    "_serving_model_dir = _housepred_root.joinpath('housepred_model')\n",
    "\n",
    "# Path to the data storage\n",
    "_data_root = _tfx_root.joinpath('data')\n",
    "\n",
    "# Set up logging.\n",
    "absl.logging.set_verbosity(absl.logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is coming from the [Melbourne house price prediction Kaggle](https://www.kaggle.com/anthonypino/melbourne-housing-market#Melbourne_housing_FULL.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAWDATA_PATH = 'https://raw.githubusercontent.com/fabiansd/AI-workshop/master/data/melb_data.csv'\n",
    "\n",
    "_rawdata_filepath = _data_root.joinpath('melbourne.csv')\n",
    "\n",
    "urllib.request.urlretrieve(RAWDATA_PATH, _rawdata_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head {_rawdata_filepath}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we create an InteractiveContext using default parameters. This will\n",
    "# use a temporary directory with an ephemeral ML Metadata database instance.\n",
    "# To use your own pipeline root or database, the optional properties\n",
    "# `pipeline_root` and `metadata_connection_config` may be passed to\n",
    "# InteractiveContext. Calls to InteractiveContext are no-ops outside of the\n",
    "# notebook.\n",
    "context = InteractiveContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ExampleGen\n",
    "\n",
    "This component takes the input data path to your data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CsvExampleGen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2e0190c2dd16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexample_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCsvExampleGen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexternal_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_data_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CsvExampleGen' is not defined"
     ]
    }
   ],
   "source": [
    "example_gen = CsvExampleGen(input=external_input(_data_root))\n",
    "context.run(example_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This component produces training and evaluation examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact = example_gen.outputs['examples'].get()[0]\n",
    "print(artifact.split_names, artifact.uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the first 3 training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the URI of the output artifact representing the training examples, which is a directory\n",
    "train_uri = os.path.join(example_gen.outputs['examples'].get()[0].uri, 'train')\n",
    "\n",
    "# Get the list of files in this directory (all compressed TFRecord files)\n",
    "tfrecord_filenames = [os.path.join(train_uri, name)\n",
    "                      for name in os.listdir(train_uri)]\n",
    "\n",
    "# Create a `TFRecordDataset` to read these files\n",
    "dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n",
    "\n",
    "# Iterate over the first 3 records and decode them.\n",
    "for tfrecord in dataset.take(3):\n",
    "  serialized_example = tfrecord.numpy()\n",
    "  example = tf.train.Example()\n",
    "  example.ParseFromString(serialized_example)\n",
    "  pp.pprint(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StatisticsGen\n",
    "\n",
    "This comp cpmputes statistics over yout dataset for data analysis. It uses the [TensorFlow Data Validation](https://www.tensorflow.org/tfx/data_validation/get_started) library. After it has been run we can display the statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_gen = StatisticsGen(\n",
    "    examples=example_gen.outputs['examples'])\n",
    "context.run(statistics_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.show(statistics_gen.outputs['statistics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SchemaGen\n",
    "\n",
    "Creates a schema based on your data statistics. It takes the statistics generated by StatisticsGen as input looking at the training split by default (which is train: 2/3 and eval: 1/3 by default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_gen = SchemaGen(\n",
    "    statistics=statistics_gen.outputs['statistics'],\n",
    "    infer_feature_shape=False)\n",
    "context.run(schema_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SchemaGen displays all your feature in a column alongside its properties and the domain of categorical values. Learn more about schemas in the [SchemaGen doc](https://www.tensorflow.org/tfx/guide/schemagen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.show(schema_gen.outputs['schema'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ExampleValidator\n",
    "\n",
    "Detects anomalies in the data, based on the expectations defined by the schema. The ExampleValidator component will input the statistics from SchemaGen and the schema from SchemaGen.\n",
    "\n",
    "By default, it compares the statistics from the evaluation split to the schema from the training split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_validator = ExampleValidator(\n",
    "    statistics=statistics_gen.outputs['statistics'],\n",
    "    schema=schema_gen.outputs['schema'])\n",
    "context.run(example_validator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then visualize anomalies as a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.show(example_validator.outputs['anomalies'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform\n",
    "\n",
    "Performs feature engineering for both training and serving. It uses the [TensorFlow Transform](https://www.tensorflow.org/tfx/transform/get_started).\n",
    "\n",
    "Transform takes the data from ExampleGen, the schema from SchemaGen and a python module that contains user-defined Transform code.\n",
    "\n",
    "First, we define a few constants for feature engineering in the prediction_constants.py file. Note: The %%writefile cell magic will save the contents of the cell as a .py file on disk. This allows the Transform component to load your code as a module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing {_melbourne_constants_module_file}\n"
     ]
    }
   ],
   "source": [
    "%%writefile {_melbourne_constants_module_file}\n",
    "\n",
    "# Categorical features are assumed to each have a maximum value in the dataset.\n",
    "MAX_CATEGORICAL_FEATURE_VALUES = [24, 31, 12]\n",
    "\n",
    "CATEGORICAL_FEATURE_KEYS = [\n",
    "    'Suburb',\n",
    "    'Address',\n",
    "    'Type',\n",
    "    'Method',\n",
    "    'SellerG', \n",
    "    'Date',\n",
    "    'CouncilArea',\n",
    "    'Regionname'\n",
    "]\n",
    "\n",
    "# Numerical features\n",
    "DENSE_FLOAT_FEATURE_KEYS = ['Rooms',\n",
    "                            'Price', \n",
    "                            'Distance',\n",
    "                            'Postcode',\n",
    "                            'Bedroom2',\n",
    "                            'Bathroom',\n",
    "                            'Car',\n",
    "                            'Landsize',\n",
    "                            'BuildingArea',\n",
    "                            'YearBuilt']\n",
    "\n",
    "# Number of buckets used by tf.transform for encoding numerical features into a bucket generaliation\n",
    "FEATURE_BUCKET_COUNT = 10\n",
    "\n",
    "BUCKET_FEATURE_KEYS = [\n",
    "    'Lattitude', \n",
    "    'Longtitude'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "_melbourne_transform_module_file = 'melbourne_transform.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
